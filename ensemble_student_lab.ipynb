{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpQBugntGsnq"
      },
      "source": [
        "# Ensemble Methods & Boosting — Student Lab\n",
        "\n",
        "Week 4 introduces sklearn models, but you must still explain *why* they work (bias/variance)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rQdqHKbtGsnu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification, load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BnfQN6-Gsnv"
      },
      "source": [
        "## Section 0 — Dataset (synthetic default, real optional)\n",
        "\n",
        "### Task 0.1: Choose dataset\n",
        "Use synthetic by default. Optionally switch to breast cancer dataset.\n",
        "\n",
        "# TODO: set `use_real = False` or True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eldEIqa0Gsnv",
        "outputId": "38ea5bab-5f33-49f9-f578-035f66f3ba84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK: shapes\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1400, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "use_real = False  # TODO\n",
        "\n",
        "if use_real:\n",
        "    data = load_breast_cancer()\n",
        "    X = data.data\n",
        "    y = data.target\n",
        "else:\n",
        "    X, y = make_classification(\n",
        "        n_samples=2000,\n",
        "        n_features=20,\n",
        "        n_informative=8,\n",
        "        n_redundant=4,\n",
        "        class_sep=1.0,\n",
        "        flip_y=0.03,\n",
        "        random_state=0,\n",
        "    )\n",
        "\n",
        "Xtr, Xva, ytr, yva = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)\n",
        "check('shapes', Xtr.shape[0]==ytr.shape[0] and Xva.shape[0]==yva.shape[0])\n",
        "Xtr.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7v4ia0IGsnv"
      },
      "source": [
        "## Section 1 — Baseline vs Trees vs Random Forest\n",
        "\n",
        "### Task 1.1: Train baseline decision tree vs random forest\n",
        "\n",
        "# TODO: Train:\n",
        "- DecisionTreeClassifier(max_depth=?)\n",
        "- RandomForestClassifier(n_estimators=?, max_depth=?, oob_score=True, bootstrap=True)\n",
        "\n",
        "Compute accuracy + ROC-AUC on validation.\n",
        "\n",
        "**Checkpoint:** Why does bagging reduce variance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJjgPMM8Gsnw",
        "outputId": "2afdd982-24c9-47c8-85f5-76318c99571b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/ensemble/_forest.py:612: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Depth 2, n_trees: 1\n",
            "tree (0.78, np.float64(0.8372999999999999))\n",
            "rf   (0.79, np.float64(0.8346333333333332))\n",
            "rf oob_score 0.5971428571428572\n",
            "Depth 2, n_trees: 25\n",
            "tree (0.78, np.float64(0.8372999999999999))\n",
            "rf   (0.8233333333333334, np.float64(0.9038444444444445))\n",
            "rf oob_score 0.8414285714285714\n",
            "Depth 2, n_trees: 100\n",
            "tree (0.78, np.float64(0.8372999999999999))\n",
            "rf   (0.8116666666666666, np.float64(0.8922888888888889))\n",
            "rf oob_score 0.845\n",
            "Depth 2, n_trees: 500\n",
            "tree (0.78, np.float64(0.8372999999999999))\n",
            "rf   (0.8216666666666667, np.float64(0.8971222222222223))\n",
            "rf oob_score 0.8571428571428571\n",
            "Depth 10, n_trees: 1\n",
            "tree (0.85, np.float64(0.8507499999999999))\n",
            "rf   (0.795, np.float64(0.7982277777777778))\n",
            "rf oob_score 0.6264285714285714\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/ensemble/_forest.py:612: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Depth 10, n_trees: 25\n",
            "tree (0.85, np.float64(0.8507499999999999))\n",
            "rf   (0.875, np.float64(0.9435611111111111))\n",
            "rf oob_score 0.905\n",
            "Depth 10, n_trees: 100\n",
            "tree (0.85, np.float64(0.8507499999999999))\n",
            "rf   (0.885, np.float64(0.9474666666666666))\n",
            "rf oob_score 0.9107142857142857\n",
            "Depth 10, n_trees: 500\n",
            "tree (0.85, np.float64(0.8507499999999999))\n",
            "rf   (0.8833333333333333, np.float64(0.9502222222222222))\n",
            "rf oob_score 0.9092857142857143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/ensemble/_forest.py:612: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Depth 20, n_trees: 1\n",
            "tree (0.85, np.float64(0.85))\n",
            "rf   (0.8066666666666666, np.float64(0.8066666666666668))\n",
            "rf oob_score 0.6192857142857143\n",
            "Depth 20, n_trees: 25\n",
            "tree (0.85, np.float64(0.85))\n",
            "rf   (0.8883333333333333, np.float64(0.9501555555555555))\n",
            "rf oob_score 0.8892857142857142\n",
            "Depth 20, n_trees: 100\n",
            "tree (0.85, np.float64(0.85))\n",
            "rf   (0.8833333333333333, np.float64(0.9495444444444444))\n",
            "rf oob_score 0.9092857142857143\n",
            "Depth 20, n_trees: 500\n",
            "tree (0.85, np.float64(0.85))\n",
            "rf   (0.88, np.float64(0.9524555555555555))\n",
            "rf oob_score 0.9107142857142857\n",
            "Depth 50, n_trees: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/ensemble/_forest.py:612: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tree (0.85, np.float64(0.85))\n",
            "rf   (0.8066666666666666, np.float64(0.8066666666666668))\n",
            "rf oob_score 0.6192857142857143\n",
            "Depth 50, n_trees: 25\n",
            "tree (0.85, np.float64(0.85))\n",
            "rf   (0.8883333333333333, np.float64(0.9501555555555555))\n",
            "rf oob_score 0.8892857142857142\n",
            "Depth 50, n_trees: 100\n",
            "tree (0.85, np.float64(0.85))\n",
            "rf   (0.8833333333333333, np.float64(0.9495444444444444))\n",
            "rf oob_score 0.9092857142857143\n",
            "Depth 50, n_trees: 500\n",
            "tree (0.85, np.float64(0.85))\n",
            "rf   (0.88, np.float64(0.9524444444444444))\n",
            "rf oob_score 0.91\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "\n",
        "def eval_model(clf, X, y):\n",
        "    pred = clf.predict(X)\n",
        "    acc = accuracy_score(y, pred)\n",
        "    # many sklearn classifiers have predict_proba; handle if not\n",
        "    if hasattr(clf, 'predict_proba'):\n",
        "        proba = clf.predict_proba(X)[:, 1]\n",
        "        auc = roc_auc_score(y, proba)\n",
        "    else:\n",
        "        auc = float('nan')\n",
        "    return acc, auc\n",
        "\n",
        "max_depths = [2,10,20,50]\n",
        "num_trees = [1,25,100,500]\n",
        "\n",
        "for max_depth in max_depths:\n",
        "    tree = DecisionTreeClassifier(max_depth=max_depth, random_state=0)\n",
        "    for n_trees in num_trees:\n",
        "        rf = RandomForestClassifier(\n",
        "            n_estimators=n_trees, max_depth=max_depth,\n",
        "            oob_score=True, bootstrap=True, random_state=0\n",
        "        )\n",
        "        tree.fit(Xtr, ytr)\n",
        "        rf.fit(Xtr, ytr)\n",
        "        print(f\"Depth {max_depth}, n_trees: {n_trees}\")\n",
        "        print('tree', eval_model(tree, Xva, yva))\n",
        "        print('rf  ', eval_model(rf, Xva, yva))\n",
        "        if hasattr(rf, 'oob_score_'):\n",
        "            print('rf oob_score', rf.oob_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3s-O656Gsnw"
      },
      "source": [
        "### Task 1.2: Feature importance gotcha\n",
        "\n",
        "Inspect `feature_importances_` and explain why correlated features can distort importances.\n",
        "\n",
        "# TODO: print top 10 features by importance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pnd_CBXoGsnw",
        "outputId": "dd1725a5-05ea-4b42-8cbf-d8d5670390e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top idx [ 3 12 15 17  7 11  4 18  1 13]\n",
            "top importances [0.13177789 0.12478299 0.11547313 0.11104177 0.09557517 0.09329674\n",
            " 0.05479175 0.03673158 0.03283524 0.03067371]\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "imp = rf.feature_importances_\n",
        "top = np.argsort(-imp)[:10]\n",
        "print('top idx', top)\n",
        "print('top importances', imp[top])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0q3m51nGsnw"
      },
      "source": [
        "## Section 2 — Gradient Boosting\n",
        "\n",
        "### Task 2.1: Train GradientBoostingClassifier\n",
        "\n",
        "# TODO: Train GB with different n_estimators and learning_rate and compare.\n",
        "\n",
        "**Checkpoint:** Why can boosting overfit with too many estimators?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fc-vtQ9zGsnw",
        "outputId": "9d28b92e-38ee-4f56-8f48-0b36b747aa07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gb {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 2} (0.8616666666666667, np.float64(0.9393722222222223))\n",
            "gb {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 2} (0.8883333333333333, np.float64(0.9497666666666666))\n",
            "gb {'n_estimators': 200, 'learning_rate': 0.05, 'max_depth': 2} (0.8833333333333333, np.float64(0.9481888888888889))\n"
          ]
        }
      ],
      "source": [
        "settings = [\n",
        "    {'n_estimators': 50, 'learning_rate': 0.1, 'max_depth': 2},\n",
        "    {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 2},\n",
        "    {'n_estimators': 200, 'learning_rate': 0.05, 'max_depth': 2},\n",
        "]\n",
        "\n",
        "for s in settings:\n",
        "    gb = GradientBoostingClassifier(random_state=0, **s)\n",
        "    gb.fit(Xtr, ytr)\n",
        "    print('gb', s, eval_model(gb, Xva, yva))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpVCfMsvGsnw"
      },
      "source": [
        "## Section 3 — XGBoost-style knobs (conceptual)\n",
        "\n",
        "### Task 3.1: Explain what each knob does\n",
        "Write 2-3 bullets each:\n",
        "- subsample\n",
        "- colsample\n",
        "- learning rate\n",
        "- max_depth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTXjK9OGGsnw"
      },
      "source": [
        "- **subsample:** Fraction of datapoints/training rows used for trianing. this helps avoid some outliers affecting all the trees\n",
        "- **colsample:** : Fraction of features used for each tree. This helps avoid using useless features/co-related features for easier generalizability\n",
        "- **learning_rate:** Rate of change of weights -> controlling how fast the system can learn. If too low, might take too long to learn. If too high, the leanring might not converge or get sensitive to outliers\n",
        "- **max_depth:** : Maximum depth of tree - if too high, then tree can heavily overfit and if it's too low, then the tree might not learn enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9hdzJY4Gsnx"
      },
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- Baseline vs RF vs GB compared\n",
        "- OOB score discussed (if available)\n",
        "- Feature importance gotcha explained"
      ]
    }
  ]
}